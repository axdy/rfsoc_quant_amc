{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modulation Classification Training (Floating-point)\n",
    "\n",
    "Welcome to the training notebook for the modulation classification model.\n",
    "\n",
    "The goal of this notebook is to train a Convolutional Neutral Network (CNN) model to learn how to classify modulation schemes using the DeepRFSoC dataset recorded with an AMD RFSoC device.\n",
    "\n",
    "## Model Dimensions\n",
    "The CNN model is a small, 4-layer network with two convolutional layers and two fully-connected layers.\n",
    "\n",
    "<img src=\"./assets/networktopology.png\" width=\"800\" alt=\"Model Topology\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages and Confirm GPU are available\n",
    "Whether you are using CUDA or ROCm, confirm you can accelerate the training loops with a GPU. Otherwise, this can run on the CPU (slow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Select which GPU to use (if available)\n",
    "gpu = 0\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(gpu)\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the DeepRFSoC dataset\n",
    "Ensure the DeepRFSoC dataset has been downloaded to your local drive and point to it with the variable `dataset_path` below. Consult the `README.md` in this folder on instructions on how to download the DeepRFSoC dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "dataset_path = './DeepRFSoC.pkl'\n",
    "os.path.isfile(dataset_path) # Confirm the dataset path is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset\n",
    "Loop over classes and noise levels (SNRs) and collect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "classes = ['QPSK','BPSK','QAM16','QAM64','PSK8','PAM4','GFSK','CPFSK']\n",
    "snrs = ['-20','-16','-12','-8','-4','0','4','8','12','16','20','24', '28', '30']\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "with open(dataset_path,'rb') as f:\n",
    "    Xd = pickle.load(f)\n",
    "mods = classes\n",
    "snrs = snrs\n",
    "X = []\n",
    "lbl = []\n",
    "snr_lbl = []\n",
    "for mod in mods:\n",
    "    for snr in snrs:\n",
    "        tmp = Xd[mod,snr]\n",
    "        X.append(tmp)\n",
    "        for i in range(Xd[mod,snr].shape[2]):\n",
    "            lbl.append(mods.index(mod))\n",
    "            snr_lbl.append(snr)\n",
    "X = np.dstack(X)\n",
    "X = np.moveaxis(X,2,0)\n",
    "labels = lbl\n",
    "dataset_values = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch Dataset Class\n",
    "Create `Dataset` class for easy access to frames during our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validation_split = 0.2\n",
    "test_split = 0.1\n",
    "shuffle_dataset = True\n",
    "# Creating data indices for training, validation, and test splits:\n",
    "dataset_size = len(labels)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor((validation_split+test_split) * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(1234)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_test_indices = indices[split:], indices[:split]\n",
    "valid_split = int(np.floor(validation_split * len(val_test_indices)))\n",
    "val_indices, test_indices = val_test_indices[valid_split:], val_test_indices[:valid_split]\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "valid_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "test_sampler = torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "\n",
    "class AMCDataset(Dataset):\n",
    "    def __init__(self, dataset, labels):\n",
    "        super(AMCDataset,self).__init__()\n",
    "        self.classes = ['QPSK','BPSK','QAM16','QAM64','PSK8','PAM4','GFSK','CPFSK']\n",
    "        self.snrs = ['-20','-16','-12','-8','-4','0','4','8','12','16','20','24','28','30']\n",
    "        self.dataset, self.labels = dataset, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx,:,:]\n",
    "        label = self.labels[idx]\n",
    "        # return data, label\n",
    "        return torch.from_numpy(data.astype(np.float32).reshape(1,2,-1)), torch.tensor(label)\n",
    "    \n",
    "dataset = AMCDataset(dataset_values, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define AMC Model\n",
    "\n",
    "Four layer CNN model with floating-point weights and activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class AMC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AMC, self).__init__()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.conv_layers.append(nn.Identity())\n",
    "        self.conv_layers.append(nn.Conv2d(\n",
    "            kernel_size=(1,3),\n",
    "            in_channels=1,\n",
    "            out_channels=64,\n",
    "            bias=False,\n",
    "        ))\n",
    "        self.conv_layers.append(nn.ReLU())\n",
    "        self.conv_layers.append(nn.Conv2d(\n",
    "            kernel_size=(2,3),\n",
    "            in_channels=64,\n",
    "            out_channels=16,\n",
    "            bias=False,\n",
    "        ))\n",
    "        self.conv_layers.append(nn.ReLU())\n",
    "        self.linear_layers.append(nn.Linear(\n",
    "            in_features=1984,\n",
    "            out_features=128,\n",
    "            bias=False,\n",
    "        ))\n",
    "        self.linear_layers.append(nn.ReLU())\n",
    "        self.linear_layers.append(nn.Linear(\n",
    "            in_features=128,\n",
    "            out_features=8,\n",
    "            bias=False,\n",
    "        ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for mod in self.conv_layers:\n",
    "            x = mod(x)\n",
    "        if len(x.shape) > 3:\n",
    "            x = x.transpose(1,3).flatten(start_dim=1)\n",
    "        else:\n",
    "            x = x.transpose(0,2).flatten(start_dim=0)\n",
    "        for mod in self.linear_layers:\n",
    "            x = mod(x)\n",
    "        return x\n",
    "    \n",
    "model = AMC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, and Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    # ensure model is in training mode\n",
    "    model.train()    \n",
    "\n",
    "    for (inputs, label) in tqdm(train_loader, desc=\"Batches\", leave=False):   \n",
    "        if gpu is not None:\n",
    "            inputs = inputs.cuda()\n",
    "            label = label.cuda()\n",
    "        model.train()\n",
    "        criterion.train()\n",
    "        # print(f\"Inputs:{inputs.shape}. label:{label}\")\n",
    "        # forward pass\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        # backward pass + run optimizer to update weights\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # model.clip_weights(-1,1)\n",
    "        # keep track of loss value\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "           \n",
    "    return losses\n",
    "\n",
    "def valid(model, val_loader, criterion):    \n",
    "    # ensure model is in eval mode\n",
    "    losses = []\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs, label) in val_loader:\n",
    "            if gpu is not None:\n",
    "                inputs = inputs.cuda()\n",
    "                label = label.cuda()\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, label)\n",
    "            losses.append(loss.cpu().detach().numpy())\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            y_true.extend(label.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred), losses\n",
    "\n",
    "def test(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs, label) in test_loader:\n",
    "            if gpu is not None:\n",
    "                inputs = inputs.cuda()\n",
    "                label = label.cuda()\n",
    "            output = model(inputs)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            y_true.extend(label.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "def display_loss_plot(losses, title=\"Training loss\", xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    go.Figure([go.Scatter(y=losses)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "With a cross entropy loss function and an Adam optimiser, the network is trained across 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "name = 'float_deeprfsoc'\n",
    "batch_size = 128\n",
    "num_epochs = 100\n",
    "patience = 8\n",
    "\n",
    "data_loader_train = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "data_loader_valid = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "data_loader_test = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "# save the losses for each q\n",
    "training_loss = {}\n",
    "validation_loss = {}\n",
    "\n",
    "model = AMC()\n",
    "if gpu is not None:\n",
    "    model = model.cuda()\n",
    "\n",
    "# loss criterion and optimiser\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if gpu is not None:\n",
    "    criterion = criterion.cuda()\n",
    "# Backup optimiser:\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "running_loss = []\n",
    "running_val_loss = []\n",
    "running_test_acc = []\n",
    "min_val_loss = np.inf\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    loss_epoch = train(model, data_loader_train, optimiser, criterion)\n",
    "    val_acc, val_loss = valid(model, data_loader_test, criterion)\n",
    "    print(\"Epoch %d: Training loss = %f, Validation loss = %f,val accuracy = %f\" % (epoch, np.mean(loss_epoch), np.mean(val_loss), val_acc))\n",
    "    mean_val_loss = np.mean(val_loss)\n",
    "    if min_val_loss > mean_val_loss:\n",
    "        print(f'Val loss decreased({min_val_loss:.6f} -> {mean_val_loss:.6f})\\t Saving Model...')\n",
    "        min_val_loss = mean_val_loss\n",
    "        # Saving State Dict\n",
    "        save_path = f'saved_models_fixed/saved_model_{name}.path'\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        count = 0\n",
    "    else:\n",
    "        if count < patience:\n",
    "            count += 1\n",
    "        else:\n",
    "            print('Early Stopping Triggered!')\n",
    "            break\n",
    "    running_loss.append(np.mean(loss_epoch))\n",
    "    running_val_loss.append(np.mean(val_loss))\n",
    "    running_test_acc.append(val_acc)\n",
    "training_loss = running_loss\n",
    "validation_loss = running_val_loss\n",
    "with open(f'loss_metrics/training_loss_{name}.pkl','wb') as f:\n",
    "    pickle.dump(training_loss,f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f'loss_metrics/validation_loss_{name}.pkl','wb') as f:\n",
    "    pickle.dump(validation_loss,f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Resulting Trainged Model\n",
    "\n",
    "Testing the performance of the model in classifying modulation schemes across varying Signal-to-Noise Ratios (SNRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'float_deeprfsoc'\n",
    "classes = mods\n",
    "device = torch.device('cpu')\n",
    "X_test = dataset_values[test_indices,:,:]\n",
    "Y_test = np.array(labels)[test_indices]\n",
    "accs = {}\n",
    "test_model = AMC()\n",
    "test_model.load_state_dict(torch.load(f'saved_models_fixed/saved_model_{name}.path', map_location=device))\n",
    "test_model.eval()\n",
    "acc = {}\n",
    "for snr in snrs:\n",
    "    # extract classes @ SNR\n",
    "    test_SNRs = list(map(lambda x: snr_lbl[x], test_indices))\n",
    "    test_X_i = X_test[np.where(np.array(test_SNRs)==snr)]\n",
    "    test_Y_i = Y_test[np.where(np.array(test_SNRs)==snr)]\n",
    "    # conf matrix\n",
    "    conf = np.zeros([len(classes),len(classes)])\n",
    "    confnorm = np.zeros([len(classes),len(classes)])\n",
    "    # estimate classes\n",
    "    in_model = np.reshape(test_X_i, (-1,1,2,128))\n",
    "    for i in range(0,in_model.shape[0]):\n",
    "        in_reshape = np.reshape(in_model[i,:,:,:],(-1,1,2,128))\n",
    "        test_Y_i_hat = test_model(torch.from_numpy(in_model[i,:,:,:]).to(torch.float32)).cpu().detach().numpy()\n",
    "        j = test_Y_i[i]\n",
    "        k = int(np.argmax(test_Y_i_hat))\n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,len(classes)):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "    cor = np.sum(np.diag(conf))\n",
    "    ncor = np.sum(conf) - cor\n",
    "    print(f'{snr}dB SNR. Overall Accuracy: {cor / (cor+ncor)}')\n",
    "    acc[snr] = 1.0*cor/(cor+ncor)\n",
    "    for i, mod in enumerate(classes):\n",
    "        acc[(mod, snr)] = conf[i,i] / np.sum(conf[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test accuracy\n",
    "with open(f'test_results/test_accuracy_{name}.pkl','wb') as f:\n",
    "    pickle.dump(acc,f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a Test set for MATLAB\n",
    "Saving a test set for the highest SNR to build hardware model in MATLAB/Simulink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make test set of only 30dB SNR\n",
    "X_test = dataset_values[test_indices,:,:]\n",
    "Y_test = np.array(labels)[test_indices]\n",
    "test_SNRs = list(map(lambda x: snr_lbl[x], test_indices))\n",
    "test_X_i = X_test[np.where(np.array(test_SNRs)=='30')]\n",
    "test_Y_i = Y_test[np.where(np.array(test_SNRs)=='30')]\n",
    "\n",
    "from scipy.io import savemat\n",
    "name = 'float_deeprfsoc'\n",
    "inputs = np.reshape(test_X_i, (-1,1,2,128))\n",
    "labels = test_Y_i\n",
    "datadict = {'inputs': inputs, 'labels': labels}\n",
    "savemat(f'matlab_saved_models/inputs_amc_{name}_30dB.mat', datadict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Accuracy vs SNR\n",
    "SNR vs Accuracy for the trained model. We can confirm the model has learned how to classify models if the classification performance increases as the noise levels decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([go.Scatter(x=list(map(int,snrs)), y=[acc[snr]*100 for snr in snrs], mode='lines+markers', marker=dict(symbol='circle-open', size=8, color='#8ecddd'))])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Accuracy vs SNR for float model',\n",
    "    xaxis=dict(title='SNR (dB)'),\n",
    "    yaxis=dict(title='Accuracy (%)',range=[0, 100]),\n",
    "    legend_title='Quantised model',\n",
    "    font=dict(family='Arial', size=16, color='black'),\n",
    "    margin=dict(l=50, r=50, t=50, b=50),\n",
    "    # paper_bgcolor='rgba(0,0,0,0)',\n",
    "    template='plotly_white',\n",
    "    height=600,\n",
    "    width=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export weights to MATLAB\n",
    "Working with the base `torch` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "model = model.cpu()\n",
    "\n",
    "Wconv1 = model.conv_layers[1].weight.detach().numpy()\n",
    "Wconv2 = model.conv_layers[3].weight.detach().numpy()\n",
    "Wdense1 = model.linear_layers[0].weight.detach().numpy()\n",
    "Wdense2 = model.linear_layers[2].weight.detach().numpy()\n",
    "mdict = {'Wconv1': Wconv1, 'Wconv2': Wconv2, 'Wdense1': Wdense1, 'Wdense2': Wdense2}\n",
    "savemat(f'matlab_saved_models/model_{name}.mat', mdict)\n",
    "# save input for testing\n",
    "inputs = np.reshape(X_test, (-1,1,2,128))\n",
    "labels = Y_test\n",
    "datadict = {'inputs': inputs, 'labels': labels}\n",
    "savemat(f'matlab_saved_models/inputs_amc_{name}.mat', datadict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radioml-BQZXSLVs-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
