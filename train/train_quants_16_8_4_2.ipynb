{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modulation Classification Training (Fixed-point)\n",
    "\n",
    "Welcome to the training notebook for the modulation classification model using Quantised-Aware Training (QAT).\n",
    "\n",
    "The goal of this notebook is to train a Convolutional Neutral Network (CNN) model to learn how to classify modulation schemes using the DeepRFSoC dataset recorded with an AMD RFSoC device.\n",
    "\n",
    "## Model Dimensions\n",
    "The CNN model is a small, 4-layer network with two convolutional layers and two fully-connected layers.\n",
    "\n",
    "<img src=\"./assets/networktopology.png\" width=\"800\" alt=\"Model Topology\">\n",
    "\n",
    "## Quantisation Parameters\n",
    "Since we are aiming to implement our trained model onto the FPGA while quantising our weights and activations. We can help the model learn to adjust for the loss in precision due to the quantisation process.\n",
    "\n",
    "Four models are trained and compared against eachother configured with varying weights quantisations.\n",
    "\n",
    "| Model  | Weight Precision   | Activation Precision |\n",
    "|--------|--------------------|----------------------|\n",
    "| 16w16a | 16-bit Fixed-point | 16-bit Fixed-point   |\n",
    "| 8w16a  | 8-bit Fixed-point  | 16-bit Fixed-point   |\n",
    "| 4w16a  | 4-bit Fixed-point  | 16-bit Fixed-point   |\n",
    "| 2w16a  | 2-bit Fixed-point  | 16-bit Fixed-point   |\n",
    "\n",
    "We are varying the precision of our weights to analyse how low the precision can be pushed. The activation precisions are maintained at 16-bits as this is where our signal information is held. The AMD RFSoC receiver produces I/Q samples at 16-bit, so we have decided to maintain the precision of the information signal.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages and Confirm GPU are available\n",
    "Whether you are using CUDA or ROCm, confirm you can accelerate the training loops with a GPU. Otherwise, this can run on the CPU (slow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Select which GPU to use (if available)\n",
    "gpu = 0\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(gpu)\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the DeepRFSoC dataset\n",
    "Ensure the DeepRFSoC dataset has been downloaded to your local drive and point to it with the variable `dataset_path` below. Consult the `README.md` in this folder on instructions on how to download the DeepRFSoC dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "dataset_path = './DeepRFSoC.pkl'\n",
    "os.path.isfile(dataset_path) # Confirm the dataset path is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset\n",
    "Loop over classes and noise levels (SNRs) and collect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(dataset_path, 'rb') as f:\n",
    "#     dataset = pickle.load(f)\n",
    "classes = ['QPSK','BPSK','QAM16','QAM64','PSK8','PAM4','GFSK','CPFSK']\n",
    "snrs = ['-20','-16','-12','-8','-4','0','4','8','12','16','20','24', '28', '30']\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "with open(dataset_path,'rb') as f:\n",
    "    Xd = pickle.load(f)\n",
    "mods = classes\n",
    "snrs = snrs\n",
    "X = []\n",
    "lbl = []\n",
    "snr_lbl = []\n",
    "for mod in mods:\n",
    "    for snr in snrs:\n",
    "        tmp = Xd[mod,snr]\n",
    "        X.append(tmp)\n",
    "        for i in range(Xd[mod,snr].shape[2]):\n",
    "            lbl.append(mods.index(mod))\n",
    "            snr_lbl.append(snr)\n",
    "X = np.dstack(X)\n",
    "X = np.moveaxis(X,2,0)\n",
    "labels = lbl\n",
    "dataset_values = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch Dataset Class\n",
    "Create `Dataset` class for easy access to frames during our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validation_split = 0.2\n",
    "test_split = 0.1\n",
    "shuffle_dataset = True\n",
    "# Creating data indices for training, validation, and test splits:\n",
    "dataset_size = len(labels)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor((validation_split+test_split) * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(1234)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_test_indices = indices[split:], indices[:split]\n",
    "valid_split = int(np.floor(validation_split * len(val_test_indices)))\n",
    "val_indices, test_indices = val_test_indices[valid_split:], val_test_indices[:valid_split]\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "valid_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "test_sampler = torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "\n",
    "class AMCDataset(Dataset):\n",
    "    def __init__(self, dataset, labels):\n",
    "        super(AMCDataset,self).__init__()\n",
    "        self.classes = ['QPSK','BPSK','QAM16','QAM64','PSK8','PAM4','GFSK','CPFSK']\n",
    "        self.snrs = ['-20','-16','-12','-8','-4','0','4','8','12','16','20','24','28','30']\n",
    "        self.dataset, self.labels = dataset, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx,:,:]\n",
    "        label = self.labels[idx]\n",
    "        # return data, label\n",
    "        return torch.from_numpy(data.astype(np.float32).reshape(1,2,-1)), torch.tensor(label)\n",
    "    \n",
    "dataset = AMCDataset(dataset_values, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Quantised AMC Models\n",
    "\n",
    "Four separate models quantised using Brevitas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant.fixed_point import Int8ActPerTensorFixedPointMinMaxInit\n",
    "class InputQuantiser(Int8ActPerTensorFixedPointMinMaxInit):\n",
    "    bit_width = 16\n",
    "    min_val = -2.0\n",
    "    max_val = 2.0 - (2.0 ** -14)\n",
    "from brevitas.quant.fixed_point import Int8WeightPerTensorFixedPoint\n",
    "class Int16WeightPerTensorFixedPoint(Int8WeightPerTensorFixedPoint):\n",
    "    bit_width = 16\n",
    "\n",
    "from brevitas.quant.fixed_point import Int8ActPerTensorFixedPoint\n",
    "class Int16ActPerTensorFixedPoint(Int8ActPerTensorFixedPoint):\n",
    "    bit_width = 16\n",
    "\n",
    "rt = False\n",
    "\n",
    "class QuantAMC16w16a(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantAMC16w16a, self).__init__()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.conv_layers.append(qnn.QuantIdentity(act_quant=InputQuantiser, return_quant_tensor=True))\n",
    "        self.conv_layers.append(qnn.QuantConv2d(\n",
    "            kernel_size=(1,3),\n",
    "            in_channels=1,\n",
    "            out_channels=64,\n",
    "            bias=False,\n",
    "            weight_quant=Int16WeightPerTensorFixedPoint,\n",
    "            # act_quant=Int16ActPerTensorFixedPoint,\n",
    "            return_quant_tensor=True))\n",
    "        self.conv_layers.append(qnn.QuantReLU(act_quant=Int16ActPerTensorFixedPoint, return_quant_tensor=True))\n",
    "        self.conv_layers.append(qnn.QuantConv2d(\n",
    "            kernel_size=(2,3),\n",
    "            in_channels=64,\n",
    "            out_channels=16,\n",
    "            bias=False,\n",
    "            weight_quant=Int16WeightPerTensorFixedPoint,\n",
    "            # act_quant=Int16ActPerTensorFixedPoint,\n",
    "            return_quant_tensor=True))\n",
    "        self.conv_layers.append(qnn.QuantReLU(act_quant=Int16ActPerTensorFixedPoint, return_quant_tensor=True))\n",
    "        self.linear_layers.append(qnn.QuantLinear(\n",
    "            in_features=1984,\n",
    "            out_features=128,\n",
    "            bias=False,\n",
    "            weight_quant=Int16WeightPerTensorFixedPoint,\n",
    "            # act_quant=Int16ActPerTensorFixedPoint,\n",
    "            return_quant_tensor=True))\n",
    "        self.linear_layers.append(qnn.QuantReLU(act_quant=Int16ActPerTensorFixedPoint, return_quant_tensor=True))\n",
    "        self.linear_layers.append(qnn.QuantLinear(\n",
    "            in_features=128,\n",
    "            out_features=8,\n",
    "            bias=False,\n",
    "            weight_quant=Int16WeightPerTensorFixedPoint,\n",
    "            # act_quant=Int16ActPerTensorFixedPoint,\n",
    "            return_quant_tensor=rt))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for mod in self.conv_layers:\n",
    "            x = mod(x)\n",
    "        if len(x.shape) > 3:\n",
    "            x = x.transpose(1,3).flatten(start_dim=1)\n",
    "        else:\n",
    "            x = x.transpose(0,2).flatten(start_dim=0)\n",
    "        for mod in self.linear_layers:\n",
    "            x = mod(x)\n",
    "        return x\n",
    "\n",
    "model = QuantAMC16w16a()\n",
    "\n",
    "# 8-bit weight, 16-bit activation\n",
    "\n",
    "class QuantAMC8w16a(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantAMC8w16a, self).__init__()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.conv_layers.append(qnn.QuantIdentity(act_quant=InputQuantiser, return_quant_tensor=True))\n",
    "        self.conv_layers.append(qnn.QuantConv2d(\n",
    "            kernel_size=(1,3),\n",
    "            in_channels=1,\n",
    "            out_channels=64,\n",
    "            bias=False,\n",
    "            weight_quant=Int8WeightPerTensorFixedPoint,\n",
    "            # act_quant=Int16ActPerTensorFixedPoint,\n",
    "            return_quant_tensor=True))\n",
    "        self.conv_layers.append(qnn.QuantReLU(act_quant=Int16ActPerTensorFixedPoint, return_quant_tensor=True))\n",
    "        self.conv_layers.append(qnn.QuantConv2d(\n",
    "            kernel_size=(2,3),\n",
    "            in_channels=64,\n",
    "            out_channels=16,\n",
    "            bias=False,\n",
    "            weight_quant=Int8WeightPerTensorFixedPoint,\n",
    "            # act_quant=Int16ActPerTensorFixedPoint,\n",
    "            return_quant_tensor=True))\n",
    "        self.conv_layers.append(qnn.QuantReLU(act_quant=Int16ActPerTensorFixedPoint, return_quant_tensor=True))\n",
    "        self.linear_layers.append(qnn.QuantLinear(\n",
    "            in_features=1984,\n",
    "            out_features=128,\n",
    "            bias=False,\n",
    "            weight_quant=Int8WeightPerTensorFixedPoint,\n",
    "            # act_quant=Int16ActPerTensorFixedPoint,\n",
    "            return_quant_tensor=True))\n",
    "        self.linear_layers.append(qnn.QuantReLU(act_quant=Int16ActPerTensorFixedPoint, return_quant_tensor=True))\n",
    "        self.linear_layers.append(qnn.QuantLinear(\n",
    "            in_features=128,\n",
    "            out_features=8,\n",
    "            bias=False,\n",
    "            weight_quant=Int8WeightPerTensorFixedPoint,\n",
    "            # act_quant=Int16ActPerTensorFixedPoint,\n",
    "            return_quant_tensor=rt))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for mod in self.conv_layers:\n",
    "            x = mod(x)\n",
    "        if len(x.shape) > 3:\n",
    "            x = x.transpose(1,3).flatten(start_dim=1)\n",
    "        else:\n",
    "            x = x.transpose(0,2).flatten(start_dim=0)\n",
    "        for mod in self.linear_layers:\n",
    "            x = mod(x)\n",
    "        return x\n",
    "\n",
    "model = QuantAMC8w16a()\n",
    "\n",
    "# 4-bit weight, 16-bit activation\n",
    "\n",
    "from brevitas.quant.fixed_point import Int4WeightPerTensorFixedPointDecoupled\n",
    "\n",
    "class Int4WeightPerTensorFixedPoint(Int4WeightPerTensorFixedPointDecoupled):\n",
    "    bit_width = 4\n",
    "\n",
    "class QuantAMC4w16a(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantAMC4w16a, self).__init__()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.conv_layers.append(qnn.QuantIdentity(act_quant=InputQuantiser, return_quant_tensor=True))\n",
    "        self.conv_layers.append(qnn.QuantConv2d(\n",
    "            kernel_size=(1,3),\n",
    "            in_channels=1,\n",
    "            out_channels=64,\n",
    "            bias=False,\n",
    "            weight_quant=Int4WeightPerTensorFixedPoint,\n",
    "            # act_quant=Int16ActPerTensorFixedPoint,\n",
    "            return_quant_tensor=True))\n",
    "        self.conv_layers.append(qnn.QuantReLU(act_quant=Int16ActPerTensorFixedPoint, return_quant_tensor=True))\n",
    "        self.conv_layers.append(qnn.QuantConv2d(\n",
    "            kernel_size=(2,3),\n",
    "            in_channels=64,\n",
    "            out_channels=16,\n",
    "            bias=False,\n",
    "            weight_quant=Int4WeightPerTensorFixedPoint,\n",
    "            # act_quant=Int16ActPerTensorFixedPoint,\n",
    "            return_quant_tensor=True))\n",
    "        self.conv_layers.append(qnn.QuantReLU(act_quant=Int16ActPerTensorFixedPoint, return_quant_tensor=True))\n",
    "        self.linear_layers.append(qnn.QuantLinear(\n",
    "            in_features=1984,\n",
    "            out_features=128,\n",
    "            bias=False,\n",
    "            weight_quant=Int4WeightPerTensorFixedPoint,\n",
    "            # act_quant=Int16ActPerTensorFixedPoint,\n",
    "            return_quant_tensor=True))\n",
    "        self.linear_layers.append(qnn.QuantReLU(act_quant=Int16ActPerTensorFixedPoint, return_quant_tensor=True))\n",
    "        self.linear_layers.append(qnn.QuantLinear(\n",
    "            in_features=128,\n",
    "            out_features=8,\n",
    "            bias=False,\n",
    "            weight_quant=Int4WeightPerTensorFixedPoint,\n",
    "            # act_quant=Int16ActPerTensorFixedPoint,\n",
    "            return_quant_tensor=rt))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for mod in self.conv_layers:\n",
    "            x = mod(x)\n",
    "        if len(x.shape) > 3:\n",
    "            x = x.transpose(1,3).flatten(start_dim=1)\n",
    "        else:\n",
    "            x = x.transpose(0,2).flatten(start_dim=0)\n",
    "        for mod in self.linear_layers:\n",
    "            x = mod(x)\n",
    "        return x\n",
    "\n",
    "model = QuantAMC4w16a()\n",
    "\n",
    "# 2-bit weight, 16-bit activation\n",
    "\n",
    "from brevitas.quant.fixed_point import Int4WeightPerTensorFixedPointDecoupled\n",
    "\n",
    "class Int2WeightPerTensorFixedPoint(Int4WeightPerTensorFixedPointDecoupled):\n",
    "    bit_width = 2\n",
    "\n",
    "class QuantAMC2w16a(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantAMC2w16a, self).__init__()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.conv_layers.append(qnn.QuantIdentity(act_quant=InputQuantiser, return_quant_tensor=True))\n",
    "        self.conv_layers.append(qnn.QuantConv2d(\n",
    "            kernel_size=(1,3),\n",
    "            in_channels=1,\n",
    "            out_channels=64,\n",
    "            bias=False,\n",
    "            weight_quant=Int2WeightPerTensorFixedPoint,\n",
    "            # act_quant=Int16ActPerTensorFixedPoint,\n",
    "            return_quant_tensor=True))\n",
    "        self.conv_layers.append(qnn.QuantReLU(act_quant=Int16ActPerTensorFixedPoint, return_quant_tensor=True))\n",
    "        self.conv_layers.append(qnn.QuantConv2d(\n",
    "            kernel_size=(2,3),\n",
    "            in_channels=64,\n",
    "            out_channels=16,\n",
    "            bias=False,\n",
    "            weight_quant=Int2WeightPerTensorFixedPoint,\n",
    "            # act_quant=Int16ActPerTensorFixedPoint,\n",
    "            return_quant_tensor=True))\n",
    "        self.conv_layers.append(qnn.QuantReLU(act_quant=Int16ActPerTensorFixedPoint, return_quant_tensor=True))\n",
    "        self.linear_layers.append(qnn.QuantLinear(\n",
    "            in_features=1984,\n",
    "            out_features=128,\n",
    "            bias=False,\n",
    "            weight_quant=Int2WeightPerTensorFixedPoint,\n",
    "            # act_quant=Int16ActPerTensorFixedPoint,\n",
    "            return_quant_tensor=True))\n",
    "        self.linear_layers.append(qnn.QuantReLU(act_quant=Int16ActPerTensorFixedPoint, return_quant_tensor=True))\n",
    "        self.linear_layers.append(qnn.QuantLinear(\n",
    "            in_features=128,\n",
    "            out_features=8,\n",
    "            bias=False,\n",
    "            weight_quant=Int2WeightPerTensorFixedPoint,\n",
    "            # act_quant=Int16ActPerTensorFixedPoint,\n",
    "            return_quant_tensor=rt))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for mod in self.conv_layers:\n",
    "            x = mod(x)\n",
    "        if len(x.shape) > 3:\n",
    "            x = x.transpose(1,3).flatten(start_dim=1)\n",
    "        else:\n",
    "            x = x.transpose(0,2).flatten(start_dim=0)\n",
    "        for mod in self.linear_layers:\n",
    "            x = mod(x)\n",
    "        return x\n",
    "\n",
    "model = QuantAMC2w16a()\n",
    "\n",
    "# initialise all models\n",
    "\n",
    "models = [QuantAMC16w16a(), QuantAMC8w16a(), QuantAMC4w16a(), QuantAMC2w16a()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, and Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    # ensure model is in training mode\n",
    "    model.train()    \n",
    "\n",
    "    for (inputs, label) in tqdm(train_loader, desc=\"Batches\", leave=False):   \n",
    "        if gpu is not None:\n",
    "            inputs = inputs.cuda()\n",
    "            label = label.cuda()\n",
    "        model.train()\n",
    "        criterion.train()\n",
    "        # print(f\"Inputs:{inputs.shape}. label:{label}\")\n",
    "        # forward pass\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        # backward pass + run optimizer to update weights\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # model.clip_weights(-1,1)\n",
    "        # keep track of loss value\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "           \n",
    "    return losses\n",
    "\n",
    "def valid(model, val_loader, criterion):    \n",
    "    # ensure model is in eval mode\n",
    "    losses = []\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs, label) in val_loader:\n",
    "            if gpu is not None:\n",
    "                inputs = inputs.cuda()\n",
    "                label = label.cuda()\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, label)\n",
    "            losses.append(loss.cpu().detach().numpy())\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            y_true.extend(label.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred), losses\n",
    "\n",
    "def test(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs, label) in test_loader:\n",
    "            if gpu is not None:\n",
    "                inputs = inputs.cuda()\n",
    "                label = label.cuda()\n",
    "            output = model(inputs)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            y_true.extend(label.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "def display_loss_plot(losses, title=\"Training loss\", xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    go.Figure([go.Scatter(y=losses)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Quantised Models\n",
    "\n",
    "With a cross entropy loss function and an Adam optimiser, the network is trained across 100 epochs.\n",
    "\n",
    "**Warning!** The following cell trains **4** separate quantised models. Due to the extra quantisation limitations applied to the models, the time taken to converge to an answer is increased. This may take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "names = ['16w16a', '8w16a', '4w16a', '2w16a']\n",
    "batch_size = 128\n",
    "num_epochs = 100\n",
    "patience = 8\n",
    "\n",
    "data_loader_train = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "data_loader_valid = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "data_loader_test = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "# save the losses for each q\n",
    "training_loss = {}\n",
    "validation_loss = {}\n",
    "\n",
    "# loop over each model\n",
    "for model in models:\n",
    "    name = names[models.index(model)]\n",
    "    if gpu is not None:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # loss criterion and optimiser\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if gpu is not None:\n",
    "        criterion = criterion.cuda()\n",
    "    # Backup optimiser:\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    running_loss = []\n",
    "    running_val_loss = []\n",
    "    running_test_acc = []\n",
    "    min_val_loss = np.inf\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        loss_epoch = train(model, data_loader_train, optimiser, criterion)\n",
    "        val_acc, val_loss = valid(model, data_loader_test, criterion)\n",
    "        print(\"Epoch %d: Training loss = %f, Validation loss = %f,val accuracy = %f\" % (epoch, np.mean(loss_epoch), np.mean(val_loss), val_acc))\n",
    "        mean_val_loss = np.mean(val_loss)\n",
    "        if min_val_loss > mean_val_loss:\n",
    "            print(f'Val loss decreased({min_val_loss:.6f} -> {mean_val_loss:.6f})\\t Saving Model...')\n",
    "            min_val_loss = mean_val_loss\n",
    "            # Saving State Dict\n",
    "            save_path = f'saved_models_fixed/saved_model_{name}_ch6.path'\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            count = 0\n",
    "        else:\n",
    "            if count < patience:\n",
    "                count += 1\n",
    "            else:\n",
    "                print('Early Stopping Triggered!')\n",
    "                break\n",
    "        running_loss.append(np.mean(loss_epoch))\n",
    "        running_val_loss.append(np.mean(val_loss))\n",
    "        running_test_acc.append(val_acc)\n",
    "    training_loss = running_loss\n",
    "    validation_loss = running_val_loss\n",
    "    with open(f'loss_metrics/training_loss_{name}.pkl','wb') as f:\n",
    "        pickle.dump(training_loss,f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f'loss_metrics/validation_loss_{name}.pkl','wb') as f:\n",
    "        pickle.dump(validation_loss,f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models' weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['16w16a', '8w16a', '4w16a', '2w16a']\n",
    "\n",
    "for i in range(4):\n",
    "    name = names[i]\n",
    "    models[i]\n",
    "    save_path = f'saved_models_fixed/saved_model_{name}.path'\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['16w16a', '8w16a', '4w16a', '2w16a']\n",
    "device = torch.device('cpu')\n",
    "X_test = dataset_values[test_indices,:,:]\n",
    "Y_test = np.array(labels)[test_indices]\n",
    "accs = {}\n",
    "for model in models:\n",
    "    test_model = model.cpu()\n",
    "    name = names[models.index(model)]\n",
    "    print(f\"Testing Model: {name}\")\n",
    "    test_model.load_state_dict(torch.load(f'saved_models_fixed/saved_model_{name}.path', map_location=device))\n",
    "    test_model.eval()\n",
    "    accs[name] = {}\n",
    "    for snr in snrs:\n",
    "        # extract classes @ SNR\n",
    "        test_SNRs = list(map(lambda x: snr_lbl[x], test_indices))\n",
    "        test_X_i = X_test[np.where(np.array(test_SNRs)==snr)]\n",
    "        test_Y_i = Y_test[np.where(np.array(test_SNRs)==snr)]\n",
    "        # conf matrix\n",
    "        conf = np.zeros([len(classes),len(classes)])\n",
    "        confnorm = np.zeros([len(classes),len(classes)])\n",
    "        # estimate classes\n",
    "        in_model = np.reshape(test_X_i, (-1,1,2,128))\n",
    "        for i in range(0,in_model.shape[0]):\n",
    "            in_reshape = np.reshape(in_model[i,:,:,:],(-1,1,2,128))\n",
    "            test_Y_i_hat = test_model(torch.from_numpy(in_model[i,:,:,:]).to(torch.float32)).cpu().detach().numpy()\n",
    "            j = test_Y_i[i]\n",
    "            k = int(np.argmax(test_Y_i_hat))\n",
    "            conf[j,k] = conf[j,k] + 1\n",
    "        for i in range(0,len(classes)):\n",
    "            confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "        cor = np.sum(np.diag(conf))\n",
    "        ncor = np.sum(conf) - cor\n",
    "        print(f'{snr}dB SNR. Overall Accuracy: {cor / (cor+ncor)}')\n",
    "        accs[name][snr] = 1.0*cor/(cor+ncor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for name in names:\n",
    "    fig.add_trace(go.Scatter(x=snrs, y=[accs[name][snr] for snr in snrs], mode='lines+markers', name=name))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Test Accuracy vs SNR for Different Quantisation Levels\",\n",
    "    xaxis_title=\"SNR (dB)\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    yaxis=dict(range=[0,1])\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a Test set for MATLAB\n",
    "\n",
    "Saving a test set for the highest SNR to build hardware model in MATLAB/Simulink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make test set of only 30dB SNR\n",
    "X_test = dataset_values[test_indices,:,:]\n",
    "Y_test = np.array(labels)[test_indices]\n",
    "test_SNRs = list(map(lambda x: snr_lbl[x], test_indices))\n",
    "test_X_i = X_test[np.where(np.array(test_SNRs)=='30')]\n",
    "test_Y_i = Y_test[np.where(np.array(test_SNRs)=='30')]\n",
    "\n",
    "from scipy.io import savemat\n",
    "name = 'float_deeprfsoc'\n",
    "inputs = np.reshape(test_X_i, (-1,1,2,128))\n",
    "labels = test_Y_i\n",
    "datadict = {'inputs': inputs, 'labels': labels}\n",
    "savemat(f'matlab_saved_models/inputs_amc_{name}_30dB.mat', datadict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Quantised Activations in Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the interlayer signal precisions\n",
    "name = 'float_deeprfsoc'\n",
    "model = models[0] # 16w16a\n",
    "model.cpu()\n",
    "model.eval()\n",
    "identity = model.conv_layers[0]\n",
    "conv1 = model.conv_layers[1]\n",
    "relu1 = model.conv_layers[2]\n",
    "conv2 = model.conv_layers[3]\n",
    "relu2 = model.conv_layers[4]\n",
    "dense1 = model.linear_layers[0]\n",
    "relu3 = model.linear_layers[1]\n",
    "dense2 = model.linear_layers[2]\n",
    "\n",
    "# use the same input as in matlab\n",
    "from scipy.io import loadmat\n",
    "inputs_path = f'matlab_saved_models/inputs_amc_{name}_30dB.mat'\n",
    "inputs = loadmat(inputs_path)['inputs']\n",
    "inputs = torch.from_numpy(inputs).to(torch.float32)\n",
    "input_frame = inputs[1,:,:,:]\n",
    "input_frame.shape\n",
    "\n",
    "inp = input_frame\n",
    "print(f'Input: {inp[0,0,0:4]}')\n",
    "l1 = identity(inp)\n",
    "print(f'identity output {l1[0][0,0,0:4]}')\n",
    "l2 = conv1(l1)\n",
    "print(f'conv1 output {l2[0][0,0,0:4]}')\n",
    "l3 = relu1(l2)\n",
    "print(f'relu1 output {l3[0][0,0,0:4]}')\n",
    "l4 = conv2(l3)\n",
    "print(f'conv2 output {l4[0][0,0,0:4]}')\n",
    "l5 = relu2(l4)\n",
    "print(f'relu2 output {l5[0][0,0,0:4]}')\n",
    "l5_flat = l5.transpose(0,2).flatten(start_dim=0)\n",
    "print(f'flatten {l5_flat.shape}')\n",
    "l6 = dense1(l5_flat)\n",
    "print(f'dense1 output {l6[0][0:4]}')\n",
    "l7 = relu3(l6)\n",
    "print(f'relu3 output {l7[0][0:4]}')\n",
    "l8 = dense2(l7)\n",
    "print(f'dense2 output {l8}')\n",
    "\n",
    "# output of model\n",
    "output = model(inp)\n",
    "print(f'model output {output}')\n",
    "\n",
    "import math\n",
    "print(f\"Input - Q{int(l1.bit_width)+int(math.log2(l1.scale))}.{-int(math.log2(l1.scale))}\")\n",
    "print(f\"Conv1 Output - Q{int(l2.bit_width)+int(math.log2(l2.scale))}.{-int(math.log2(l2.scale))}\")\n",
    "print(f\"ReLU1 Output - Q{int(l3.bit_width)+int(math.log2(l3.scale))}.{-int(math.log2(l3.scale))}\")\n",
    "print(f\"Conv2 Output - Q{int(l4.bit_width)+int(math.log2(l4.scale))}.{-int(math.log2(l4.scale))}\")\n",
    "print(f\"ReLU2 Output - Q{int(l5.bit_width)+int(math.log2(l5.scale))}.{-int(math.log2(l5.scale))}\")\n",
    "print(f\"Flatten - Q{int(l5_flat.bit_width)+int(math.log2(l5_flat.scale))}.{-int(math.log2(l5_flat.scale))}\")\n",
    "print(f\"Dense1 Output - Q{int(l6.bit_width)+int(math.log2(l6.scale))}.{-int(math.log2(l6.scale))}\")\n",
    "print(f\"ReLU3 Output - Q{int(l7.bit_width)+int(math.log2(l7.scale))}.{-int(math.log2(l7.scale))}\")\n",
    "print(f\"Dense2 Output - Q{int(l8.bit_width)+int(math.log2(l8.scale))}.{-int(math.log2(l8.scale))}\")\n",
    "print(f\"Model Output - Q{int(output.bit_width)+int(math.log2(output.scale))}.{-int(math.log2(output.scale))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Quantised Weights\n",
    "We can inspect the fixed-point parameters of each set of weights, including the fractional bits learned for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "for model in models:\n",
    "    name = names[models.index(model)]\n",
    "    print(f\"Models: {name}\")\n",
    "    print(f\"Conv 1 - Q{model.conv_layers[1].quant_weight().bit_width}.{-math.log2(model.conv_layers[1].quant_weight().scale)}\")\n",
    "    print(f\"Conv 2 - Q{model.conv_layers[3].quant_weight().bit_width}.{-math.log2(model.conv_layers[3].quant_weight().scale)}\")\n",
    "    print(f\"FC 1 - Q{model.linear_layers[0].quant_weight().bit_width}.{-math.log2(model.linear_layers[0].quant_weight().scale)}\")\n",
    "    print(f\"FC 2 - Q{model.linear_layers[2].quant_weight().bit_width}.{-math.log2(model.linear_layers[2].quant_weight().scale)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models' weights for MATLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "names = ['16w16a', '8w16a', '4w16a', '2w16a']\n",
    "for model in models:\n",
    "    model = model.cpu()\n",
    "    name = names[models.index(model)]\n",
    "    Wconv1 = model.conv_layers[1].weight.detach().numpy()\n",
    "    Wconv2 = model.conv_layers[3].weight.detach().numpy()\n",
    "    Wdense1 = model.linear_layers[0].weight.detach().numpy()\n",
    "    Wdense2 = model.linear_layers[2].weight.detach().numpy()\n",
    "    mdict = {'Wconv1': Wconv1, 'Wconv2': Wconv2, 'Wdense1': Wdense1, 'Wdense2': Wdense2}\n",
    "    savemat(f'matlab_saved_models/model_{name}.mat', mdict)\n",
    "# save input for testing\n",
    "inputs = np.reshape(X_test, (-1,1,2,128))\n",
    "labels = Y_test\n",
    "datadict = {'inputs': inputs, 'labels': labels}\n",
    "savemat(f'matlab_saved_models/inputs_amc_deeprfsoc.mat', datadict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a Test set for Hardware\n",
    "\n",
    "Saving a separate test set for hardware so that we can load this onto the RFSoC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = dataset_values[test_indices,:,:]\n",
    "Y_test = np.array(labels)[test_indices]\n",
    "testset = {}\n",
    "for snr in snrs:\n",
    "    # extract classes @ SNR\n",
    "    test_SNRs = list(map(lambda x: snr_lbl[x], test_indices))\n",
    "    test_X_i = X_test[np.where(np.array(test_SNRs)==snr)]\n",
    "    test_Y_i = Y_test[np.where(np.array(test_SNRs)==snr)]\n",
    "    testset[snr] = (test_X_i, test_Y_i)\n",
    "import pickle\n",
    "with open(f'matlab_saved_models/testset.pkl','wb') as f:\n",
    "    pickle.dump(testset,f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect weights of deployed models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['16w16a', '8w16a', '4w16a', '2w16a']\n",
    "device = torch.device('cpu')\n",
    "X_test = dataset_values[test_indices,:,:]\n",
    "Y_test = np.array(labels)[test_indices]\n",
    "accs = {}\n",
    "for model in models:\n",
    "    model = model.cpu()\n",
    "    name = names[models.index(model)]\n",
    "    print(f\"Loading model weights for {name}\")\n",
    "    model.load_state_dict(torch.load(f'saved_models_fixed/saved_model_{name}.path', map_location=device))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radioml-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
